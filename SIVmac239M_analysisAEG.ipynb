{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIVmac239M analysis pipeline \n",
    "### Modified from Athena Golfinos, Ryan Moriarty and Brandon Keele\n",
    "January 28, 2021\n",
    "\n",
    "## Steps:\n",
    "#### 1: Download files into the proper location \n",
    "#### 2: Demultiplex files \n",
    "#### 3: Barcoded virus analysis script from Brandon Keele\n",
    "#### 4: Examination of barcode distribution using Simpson's Diversity index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import collections \n",
    "from fastqandfurious import fastqandfurious\n",
    "from fastqandfurious.fastqandfurious import entryfunc\n",
    "import regex\n",
    "import matplotlib.pyplot as plt\n",
    "import fnmatch\n",
    "import matplotlib.backends.backend_pdf\n",
    "import numpy as np\n",
    "import csv\n",
    "import itertools\n",
    "import shutil\n",
    "import scipy\n",
    "import re\n",
    "import sys\n",
    "import seaborn as sb\n",
    "from functools import reduce\n",
    "import statistics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables: User Input Necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROOT = the absolute path to the fastq file that we need\n",
    "\n",
    "EXPT = the name of the folder that we are looking at data within\n",
    "\n",
    "INDICES = name of the file that contains all of our index tags. This file should be inside the EXPT folder which is then inside the ROOT directory. Either your file should be named \"index_ids.csv\", or you will need to edit the INDICES string below (that is the portion in red) \n",
    "\n",
    "BARCODES = this is the file that contains all of the reference barcode sequences for SIVmac239M. This should be contained within the ROOT directory, in a folder called \"analysis\", and the file should be named SIV239MReferenceSequences.fasta\". If not, be sure to change the red test under \"BARCODES\" below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/Users/agolfinos/Desktop/test\"\n",
    "\n",
    "EXPT = \"\"\n",
    "\n",
    "INPUT = ROOT + EXPT + \"/\" + \"rh2903-rh2909-rh2911-allsamples_S1_L001_R_001.fastq\"\n",
    "\n",
    "INDICES = pd.read_csv(ROOT + \"/analysis/index_ids.csv\", sep = ',', header = None)\n",
    "\n",
    "#INDICES_5 = pd.read_csv(ROOT + \"analysis/index_ids_5prime.csv\", sep = ',', header = None)\n",
    "idx5 = list(zip(INDICES[0], INDICES[1], INDICES[2]))\n",
    "\n",
    "#INDICES_7 = pd.read_csv(ROOT + \"analysis/index_ids_7prime.csv\", sep = \",\", header = None)\n",
    "idx7 = list(zip(INDICES[0], INDICES[1], INDICES[2]))\n",
    "\n",
    "BARCODES = [(b.id, b.seq) for b in SeqIO.parse(ROOT + \"/analysis/SIV239MReferenceSequences.fasta\", \"fasta\")]\n",
    "\n",
    "pre_index = \"GAGGTTCTGG\"\n",
    "\n",
    "pre_index_reverse_complement = \"CCAGAACCTC\"\n",
    "\n",
    "post_index = \"CTAGGGGAAG\"\n",
    "\n",
    "post_index_reverse_complement = \"CTTCCCCTAG\"\n",
    "\n",
    "os.chdir(ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the Orientation of the Reads and Reverse Transcribing Sequences, If Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastq_file_lists(INPUT):\n",
    "    # pull in the sequences \n",
    "    seqs = []\n",
    "    bufsize = 200000\n",
    "    with open(INPUT, \"rb\") as fh:\n",
    "        it = fastqandfurious.readfastq_iter(fh, bufsize, entryfunc)\n",
    "        for sequence in it:\n",
    "            # sequence[1] is the nucleotide sequence, sequence[2] is the quality scores \n",
    "            seqs.append((str(sequence[1])[2:-2], str(sequence[2])[2:-2]))\n",
    "\n",
    "    preidx = []\n",
    "    rcidx = []\n",
    "    preidx_not_found = []\n",
    "    for a in seqs:\n",
    "        if pre_index in a[0]:\n",
    "            preidx.append(a[0].index(pre_index))\n",
    "        elif pre_index_reverse_complement in a[0]: \n",
    "            rcidx.append(a[0].index(pre_index_reverse_complement))\n",
    "        else:\n",
    "            preidx_not_found.append(a)\n",
    "    seqs_rc = []\n",
    "    if len(rcidx) > len(preidx):\n",
    "        for a in seqs:\n",
    "                seqs_rc.append((str(Seq(a[0]).reverse_complement()), a[1][::-1]))\n",
    "        seqs = seqs_rc\n",
    "        print(\"samples are reverse transcribed \")\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding all of the possible pre-index identifiers (Pre-Sequence GAGGTTCTGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_indexes(INPUT):\n",
    "    \n",
    "    #this calls the list of sequences that we created in the previous section\n",
    "    all_seqs = fastq_file_lists(INPUT)\n",
    "    \n",
    "    #making an empty list to put our pre-index sequences in\n",
    "    pre_index_permutations = []\n",
    "    \n",
    "    #adding the wild type sequence\n",
    "    pre_index_permutations.append(\"GAGGTTCTGG\")\n",
    "    \n",
    "    #now we are iterating through the list of all sequences\n",
    "    for sequence in all_seqs: \n",
    "        \n",
    "        #within the sequence, find the pre-index GAGGTTCTGG sequence, but allow for one mismatch to detect mutants\n",
    "        m = regex.findall(\"(GAGGTTCTGG){s<=1}\", str(sequence[0]))\n",
    "        \n",
    "        #now we are adding this sequence to our list of pre-index permutations\n",
    "        pre_index_permutations.append(m)\n",
    "    \n",
    "    #what this is doing is flattening our 'list within lists', so making a single list that encompasses our sequences\n",
    "    flat_list = [item for sublist in pre_index_permutations for item in sublist]\n",
    "    \n",
    "    #now, we are taking this list and ONLY keeping the UNIQUE values. This gets rid of all duplicates and provides us\n",
    "    #with a list of just the unique identifiers we could find\n",
    "    flat_list_set = set(flat_list)\n",
    "    \n",
    "    #this prevents any random sequences that aren't 10 nucleotides long from sneaking into our list by specifying that it must be 10nt long\n",
    "    flat_list_set_int = [seq for seq in flat_list_set if len(seq) == 10]\n",
    "    print(\"Our unique pre-index tag sequences are:\")\n",
    "    print(flat_list_set_int)\n",
    "       \n",
    "    return(flat_list_set_int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding all possible pre-index sequences (Pre-index sequence TCTCCAGCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_indexes(INPUT):\n",
    "    \n",
    "    #this calls the list of sequences that we created in the previous section\n",
    "    all_seqs = fastq_file_lists(INPUT)\n",
    "    \n",
    "    #making an empty list to put our pre-index sequences in\n",
    "    pre_index_permutations = []\n",
    "    \n",
    "    #adding the wild type sequence\n",
    "    pre_index_permutations.append(\"TCTCCAGCT\")\n",
    "    \n",
    "    #now we are iterating through the list of all sequences\n",
    "    for sequence in all_seqs: \n",
    "        \n",
    "        #within the sequence, find the pre-index GAGGTTCTGG sequence, but allow for one mismatch to detect mutants\n",
    "        m = regex.findall(\"(TCTCCAGCT){s<=1}\", str(sequence[0]))\n",
    "        \n",
    "        #now we are adding this sequence to our list of pre-index permutations\n",
    "        pre_index_permutations.append(m)\n",
    "    \n",
    "    #what this is doing is flattening our 'list within lists', so making a single list that encompasses our sequences\n",
    "    flat_list = [item for sublist in pre_index_permutations for item in sublist]\n",
    "    \n",
    "    #now, we are taking this list and ONLY keeping the UNIQUE values. This gets rid of all duplicates and provides us\n",
    "    #with a list of just the unique identifiers we could find\n",
    "    flat_list_set = set(flat_list)\n",
    "    \n",
    "    #this prevents any random sequences that aren't 10 nucleotides long from sneaking into our list by specifying that it must be 10nt long\n",
    "    flat_list_set_int = [seq for seq in flat_list_set if len(seq) == 10]\n",
    "    print(\"Our unique pre-index tag sequences are:\")\n",
    "    print(flat_list_set_int)\n",
    "       \n",
    "    return(flat_list_set_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Index Demultiplex our FASTQ file using WT only pre-index sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demultiplexes the Miseq file and separates them into individual samples\n",
    "Args:\n",
    "   file = the non-demultiplexed R1 or R2 file, save as .fastq\n",
    "   indices = the P5.XX used to label each sample\n",
    "        \n",
    "Returns:\n",
    "   list of demultiplexed samples:\n",
    "   \"Unknown_tag\" - these are samples where the pre-index sequence \"GAGGTTCTGG\" or its reverese complement \n",
    "    was found, but the subseequent 8 base pairs, where the tag should be, was not present in the list of known \n",
    "    index tags. \n",
    "    \"Unindexed\" - these are samples where the pre-index sequence \"GAGGTTCTGG\" or its reverse complement was \n",
    "    NOT found, so we didn't look for the index tag either.\n",
    "    All other samples will be labeled with their index tag \"P5.XX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demultiplex(INPUT, INDICES, write_files=True):\n",
    "    \n",
    "    file_end = INPUT[INPUT.rfind(\"/\")+1:INPUT.rfind(\".fastq\")]\n",
    "    all_seqs = fastq_file_lists(INPUT)\n",
    "    \n",
    "    index_names = list(INDICES[0])\n",
    "    index_seqs = list(INDICES[1])\n",
    "    index_seqs_rc = list(INDICES[2])\n",
    "    \n",
    "    # find where the indices are hiding \n",
    "    dmux_samples = dict()\n",
    "    for a in all_seqs:\n",
    "        if pre_index in a[0]:\n",
    "            start = a[0].index(pre_index)+10\n",
    "            tag = a[0][start:start+8]\n",
    "            if tag in index_seqs:\n",
    "                name = index_names[index_seqs.index(tag)] + \"_rc\"\n",
    "                dmux_samples.setdefault(name, []).append(a)\n",
    "            elif tag in index_seqs_rc:\n",
    "                name = index_names[index_seqs_rc.index(tag)] \n",
    "                dmux_samples.setdefault(name, []).append(a)\n",
    "            else:\n",
    "                dmux_samples.setdefault(\"Unknown_tag\", []).append(a)\n",
    "        elif pre_index_reverse_complement in a[0]:\n",
    "            start = a[0].index(pre_index_reverse_complement)+10\n",
    "            tag = a[0][start:start+8]\n",
    "            if tag in index_seqs:\n",
    "                name = index_names[index_seqs.index(tag)] + \"_rc\"\n",
    "                dmux_samples.setdefault(name, []).append(a) \n",
    "            elif tag in index_seqs_rc:\n",
    "                name = index_names[index_seqs_rc.index(tag)] \n",
    "                dmux_samples.setdefault(name, []).append(a)\n",
    "            else:\n",
    "                dmux_samples.setdefault(\"Unknown_tag\", []).append(a)\n",
    "        else:\n",
    "            dmux_samples.setdefault(\"Unindexed\", []).append(a)\n",
    "\n",
    "    index_keys = list(dmux_samples.keys())\n",
    "    real_tags_found = len(list(filter(lambda element: 'P' in element, index_keys)))\n",
    "\n",
    "    if real_tags_found > 0:\n",
    "        if write_files:\n",
    "            for i in index_keys:\n",
    "                print(\"Writing sample index: \", i)\n",
    "                data = dmux_samples.get(i)\n",
    "                sample_fastq = open(file_end+ \"_\" + i + \"_demultiplexed.fastq\", \"w\")\n",
    "                l = 0\n",
    "                for line in data:\n",
    "                    l += 1\n",
    "                    sample_fastq.write(\"@\" + i + \".\" + str(l) + \"\\n\" + line[0] + \"\\n+\\n\" + line[1] + \"\\n\" )\n",
    "                sample_fastq.close()\n",
    "    else:\n",
    "        print(\"No known index tags found, try different read.\")\n",
    "\n",
    "    return dmux_samples\n",
    "\n",
    "#print(fastqs[23])\n",
    "demultiplexed = demultiplex(INPUT, INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Index Demultiplex using a list of potential pre-index sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demultiplex(INPUT, INDICES, write_files=True):\n",
    "    \n",
    "    pre_index_list = pre_indexes(INPUT)\n",
    "    file_end = INPUT[INPUT.rfind(\"/\")+1:INPUT.rfind(\".fastq\")]\n",
    "    all_seqs = fastq_file_lists(INPUT)\n",
    "    \n",
    "    index_names = list(INDICES[0])\n",
    "    index_seqs = list(INDICES[1])\n",
    "    index_seqs_rc = list(INDICES[2])\n",
    "    \n",
    "    # find where the indices are hiding \n",
    "    dmux_samples = dict()\n",
    "    for a in all_seqs:\n",
    "        count = 0\n",
    "        for pre_index in pre_index_list:\n",
    "            if pre_index in a[0]:\n",
    "                start = a[0].index(pre_index)+10\n",
    "                tag = a[0][start:start+8]\n",
    "                if tag in index_seqs:\n",
    "                    name = index_names[index_seqs.index(tag)] + \"_rc\"\n",
    "                    dmux_samples.setdefault(name, []).append(a)\n",
    "                elif tag in index_seqs_rc:\n",
    "                    name = index_names[index_seqs_rc.index(tag)] \n",
    "                    dmux_samples.setdefault(name, []).append(a)\n",
    "                count += 1\n",
    "            elif pre_index_reverse_complement in a[0]:\n",
    "                start = a[0].index(pre_index_reverse_complement)+10\n",
    "                tag = a[0][start:start+8]\n",
    "                if tag in index_seqs:\n",
    "                    name = index_names[index_seqs.index(tag)] + \"_rc\"\n",
    "                    dmux_samples.setdefault(name, []).append(a) \n",
    "                elif tag in index_seqs_rc:\n",
    "                    name = index_names[index_seqs_rc.index(tag)] \n",
    "                    dmux_samples.setdefault(name, []).append(a)\n",
    "                count +=1 \n",
    "        if count == 0:\n",
    "            dmux_samples.setdefault(\"Undetermined\", []).append(a)\n",
    "\n",
    "        index_keys = list(dmux_samples.keys())\n",
    "        real_tags_found = len(list(filter(lambda element: 'P' in element, index_keys)))\n",
    "\n",
    "    if real_tags_found > 0:\n",
    "        if write_files:\n",
    "            for i in index_keys:\n",
    "                print(\"Writing sample index: \", i)\n",
    "                data = dmux_samples.get(i)\n",
    "                sample_fastq = open(file_end+ \"_\" + i + \"_demultiplexed.fastq\", \"w\")\n",
    "                l = 0\n",
    "                for line in data:\n",
    "                    l += 1\n",
    "                    sample_fastq.write(\"@\" + i + \".\" + str(l) + \"\\n\" + line[0] + \"\\n+\\n\" + line[1] + \"\\n\" )\n",
    "                sample_fastq.close()\n",
    "    else:\n",
    "        print(\"No known index tags found, try different read.\")\n",
    "\n",
    "    return dmux_samples\n",
    "\n",
    "#print(fastqs[23])\n",
    "demultiplexed = demultiplex(INPUT, INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Index Demultiplexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes the following as input: \n",
    "\n",
    "*F1 = this is the \"R1\" file from your run\n",
    "*F2 = this is the \"R2\" file from your run\n",
    "*index_keys_5 = this is the absolute path to a csv file that contains the names (column 1), sequences (column 2), and reverse complement of sequences (column 3) of your P5 index sequences from the primers. \n",
    "*index_keys_7 = this is the absolute path to a csv file that contains the names (column 1), sequences (column 2), and the reverse complement of sequences (column 3) of your P7 index sequences from the primers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Index Demultiplexing Variables: User Input Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = ROOT + \"/rh2903-rh2909-rh2911-allsamples_S1_L001_R1_001.fastq\"\n",
    "F2 = ROOT + \"/rh2903-rh2909-rh2911-allsamples_S1_L001_R2_001.fastq\"\n",
    "\n",
    "index_keys_5 = pd.read_csv(ROOT + \"/analysis/index_ids_5prime.csv\", sep = ',', header = None)\n",
    "idx5 = list(zip(index_keys_5[0], index_keys_5[1], index_keys_5[2]))\n",
    "\n",
    "index_keys_7 = pd.read_csv(ROOT + \"/analysis/index_ids_7prime.csv\", sep = \",\", header = None)\n",
    "idx7 = list(zip(index_keys_7[0], index_keys_7[1], index_keys_7[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastq_file_info(F1, F2, write_files=True):\n",
    "    \n",
    "    file_end = F1[F1.rfind(\"/\")+1:INPUT.rfind(\".fastq\")]\n",
    "    \n",
    "    #creating an empty list for our dual demultiplexed samples\n",
    "    dmux_samples = dict()\n",
    "    \n",
    "    #pulling in our index names and sequences that we will use to demultiplex the R1\n",
    "    index_names_R1 = list(index_keys_5[0])\n",
    "    index_seqs_R1 = list(index_keys_5[1])\n",
    "    index_seqs_rc_R1 = list(index_keys_5[2])\n",
    "    \n",
    "    #pulling in our index names and sequences that we will use to demultiplex the R2\n",
    "    index_names_R2 = list(index_keys_7[0])\n",
    "    index_seqs_R2 = list(index_keys_7[1])\n",
    "    index_seqs_rc_R2 = list(index_keys_7[2])\n",
    "    \n",
    "    #pull in the sequences from F1\n",
    "    headers1 = []\n",
    "    seqs1 = []\n",
    "    bufsize = 200000\n",
    "    with open(F1, \"rb\") as fh:\n",
    "        it = fastqandfurious.readfastq_iter(fh, bufsize, entryfunc)\n",
    "        for sequence in it:\n",
    "            #sequence[0] is the header, sequence[1] is the nucleotide sequence, sequence[2] is the quality scores\n",
    "            headers1.append(str(sequence[0])[2:-9])\n",
    "            seqs1.append(str(sequence[1])[2:-2] + \",\" + str(sequence[2])[2:-2])    \n",
    "    #turning this list into a Pandas dataframe so we can more easily join the datasets from R1 and R2\n",
    "    df1 = pd.DataFrame(headers1, columns=['Headers'])\n",
    "    df1[\"Sequences R1\"] = seqs1\n",
    "            \n",
    "    #pull in the sequences from F2\n",
    "    headers2 = []\n",
    "    seqs2 = []\n",
    "    bufsize = 200000\n",
    "    with open(F2, \"rb\") as fh: \n",
    "        it2 =fastqandfurious.readfastq_iter(fh, bufsize, entryfunc)\n",
    "        for sequence in it2: \n",
    "            headers2.append(str(sequence[0])[2:-9])\n",
    "            seqs2.append(str(sequence[1])[2:-2] + \",\" + str(sequence[2])[2:-2])\n",
    "\n",
    "    #turning this list into a Pandas dataframe so we can more easily join the datasets from R1 and R2\n",
    "    df2 = pd.DataFrame(headers2, columns=['Headers'])\n",
    "    df2[\"Sequences R2\"] = seqs2\n",
    "    \n",
    "    #joining dataframes 1 and 2 horizontally based on headers, combining all data from one read into one row\n",
    "    df3 = pd.merge(df1, df2, on='Headers')\n",
    "    \n",
    "    #converting this dataframe to a list so we can better iterate through certain items in the list\n",
    "    reads = df3.values.tolist()\n",
    "\n",
    "####################END DATA ORGANIZATION AND CONCATENATION################################\n",
    "####################NOW WE ARE ITERATING THROUGH OUR DATASET###############################\n",
    "   \n",
    "    #for one \"sample\"\n",
    "    for item in reads: \n",
    "        \n",
    "        #making a count of the number of real indexes found in this read--starting at zero\n",
    "        count = 0 \n",
    "        \n",
    "        #if our pre_index sequence is in the R1 sequence: \n",
    "        if pre_index in item[1]: \n",
    "            #print(item[1])\n",
    "            \n",
    "            #find the 8bp tag\n",
    "            start0 = item[1].index(pre_index)+8\n",
    "            rtag0 = item[1][start0:start0+8]            \n",
    "            \n",
    "            #if this tag is in our list of tags\n",
    "            if rtag0 in index_seqs_R1: \n",
    "            \n",
    "                #we have found a \"real\" tag, so we will add to the count for this read\n",
    "                count+=1\n",
    "                \n",
    "                #then, look for the post-index tag in the forward read (R2)\n",
    "                if post_index in item[2]:\n",
    "                    #print(item[2])\n",
    "                    \n",
    "                    #now we want to find our index tag relative to our post-index sequence\n",
    "                    start5 = item[2].index(post_index)-8\n",
    "                    tag5 = item[2][start5:start5+8]\n",
    "                    \n",
    "                    #if our identified P7 index tag is in the list of pre-defined P7 index tags\n",
    "                    if tag5 in index_seqs_R2: \n",
    "                        \n",
    "                        count+=1\n",
    "                        \n",
    "                        #we are giving our dual indexed read a name--the P5 and P7 index tags\n",
    "                        name = index_names_R1[index_seqs_rc_R1.index(tag1)] + \"_\" + index_names_R2[index_seqs_R2.index(tag2)]\n",
    "                        dmux_samples.setdefault(name, []).append(item[1])\n",
    "                elif post_index_reverse_complement in item[2]: \n",
    "                    print(\"RC post index in this sample\")\n",
    "                        \n",
    "        #if our pre index reverse complement sequence is in the R1 sequence\n",
    "        elif pre_index_reverse_complement in item[1]:\n",
    "            #print(\"Our R1 read is:\" + item[1])\n",
    "            \n",
    "            #find the 8bp tag in the R1 read\n",
    "            start1 = item[1].index(pre_index_reverse_complement)-8\n",
    "            tag1 = item[1][start1:start1+8]\n",
    "            \n",
    "            #if this tag is in our list of R1/P5 tags\n",
    "            if tag1 in index_seqs_rc_R1:\n",
    "                \n",
    "                #we have found a \"real\" P5 tag, so we will add to the count for this read\n",
    "                count+=1\n",
    "                \n",
    "                #then we want to see if our \"post index\" is in the R2 read\n",
    "                if post_index in item[2]: \n",
    "                    \n",
    "                    #now we want to find our index tag relative to our index sequence\n",
    "                    start2 = item[2].index(post_index)-8\n",
    "                    tag2 = item[2][start2:start2+8]\n",
    "                    \n",
    "                    #if our identified P7 index tag is in the list of pre-defined P7 index tags\n",
    "                    if tag2 in index_seqs_R2: \n",
    "                        \n",
    "                        #we have found a \"real\" tag, so we will add to the count for this read\n",
    "                        count+=1\n",
    "                        \n",
    "                        #we are giving our dual indexed read a name--the P5 and P7 indexes combined\n",
    "                        name = index_names_R1[index_seqs_rc_R1.index(tag1)] + \"_\" + index_names_R2[index_seqs_R2.index(tag2)] \n",
    "                        dmux_samples.setdefault(name, []).append(item[1])\n",
    "                        \n",
    "                elif post_index_reverse_complement in item[2]: \n",
    "                    print(\"RC post index in this RC sample\")\n",
    "\n",
    "        if count == 0:\n",
    "            dmux_samples.setdefault(\"Undetermined\", []).append(item[1])\n",
    "        \n",
    "        \n",
    "    \n",
    "###################GETTING A LIST OF INDEX KEYS AND THEIR COUNTS###########################\n",
    "    \n",
    "    #this provides a list of index keys (samples) as well as their corresponding read counts\n",
    "    for k,v in dmux_samples.items():\n",
    "        print (k, len(list(filter(None, v))))\n",
    "\n",
    "    #compliling a list of all our index keys, or samples to be demultiplexed\n",
    "    index_keys = list(dmux_samples.keys())\n",
    "    \n",
    "    #this counts the list of samples to be demultiplexed\n",
    "    real_tags_found = len(list(filter(lambda element: 'P' in element, index_keys)))\n",
    "\n",
    "##################IF WE HAVE REAL DATA, WE WILL NOW COMPILE IT AND WRITE TO FILES##########\n",
    "    \n",
    "    #if our list of real tags is greater than 0\n",
    "    if real_tags_found > 0:\n",
    "        if write_files:\n",
    "            \n",
    "            #for each of the index keys\n",
    "            for i in index_keys:\n",
    "                print(\"Writing sample index: \", i)\n",
    "                \n",
    "                #this is the sequence/qual scores of all the reads that fell under this index key\n",
    "                data = dmux_samples.get(i)\n",
    "                \n",
    "                #opening the fastq file for this particular index \n",
    "                sample_fastq = open(file_end + \"_\" + i + \"_demultiplexed.fastq\", \"w\")\n",
    "\n",
    "                l = 0\n",
    "                \n",
    "                #for each read + qual score entry in the data set for this index\n",
    "                for entry in data: \n",
    "                    print(entry)\n",
    "                    l += 1\n",
    "                \n",
    "                #this splits the sequence/qual score string into two parts \n",
    "                #sequence is lines[0] and qual score is lines [1]\n",
    "                    lines = entry.split(\",\")               \n",
    "                    sample_fastq.write(\"@\" + i + \".\" + str(l) + \"\\n\" + lines[0] + \"\\n+\\n\" + lines[1] + \"\\n\" )\n",
    "                sample_fastq.close()\n",
    "    else:\n",
    "        print(\"No known index tags found, try different read.\")\n",
    "\n",
    "    return dmux_samples \n",
    "\n",
    "fastq = fastq_file_info(F1, F2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------\n",
    "# Barcode Analysis (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_barcodes_v2(file_dict, barcode_file):\n",
    "    \"\"\" \n",
    "    Finds the barcodes present in the demultiplexed files \n",
    "    \n",
    "    Args:\n",
    "        files: list of demultiplexed files as a SeqIO-parsed generator\n",
    "        barcode_file: list of barcodes to search for in the samples, as a SeqIO-parsed generator \n",
    "        \n",
    "    Returns:\n",
    "        a collections.Counter object for each sample, describing {barcode: counts}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for f in file_dict:\n",
    "        if \"P\" in f:\n",
    "            print(\"Starting barcode search for \", f)\n",
    "            sample = file_dict.get(f)\n",
    "            sequence = list(zip(*sample))[0]\n",
    "            barcodes_found = []\n",
    "            for b in BARCODES:\n",
    "                if b[1] in sequence:\n",
    "                    barcodes_found.append(b[0])\n",
    "            print(f, collections.Counter(barcodes_found))\n",
    "        \n",
    "        #for sample in sample_indices:\n",
    "        #    print(sample, len(sample_seqs[sample]))\n",
    "        #    sequences = list(zip(*sample_seqs[sample]))\n",
    "        #    barcodes_found = []\n",
    "        #    for b in BARCODES:\n",
    "        #        print(b[0])\n",
    "        #        if b[1] in sequences:\n",
    "        #            barcodes_found.append(b[0])\n",
    "        #    print(collections.Counter(barcodes_found))\n",
    "       \n",
    "    \n",
    "    #return all_barcodes \n",
    "\n",
    "find_barcodes_v2(demultiplexed, BARCODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_barcodes(files, barcode_file):\n",
    "    \"\"\" \n",
    "    Finds the barcodes present in the demultiplexed files \n",
    "    \n",
    "    Args:\n",
    "        files: list of demultiplexed files as a SeqIO-parsed generator\n",
    "        barcode_file: list of barcodes to search for in the samples, as a SeqIO-parsed generator \n",
    "        \n",
    "    Returns:\n",
    "        a collections.Counter object for each sample, describing {barcode: counts}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    all_barcodes = []\n",
    "    i = 0\n",
    "    for f in files:\n",
    "        i += 1\n",
    "        print(\"Starting sample \", i) #so we are sure we are still progressing \n",
    "        barcodes_found = []\n",
    "        for sample in f:\n",
    "            for b in barcode_file:\n",
    "                if b[1] in sample.seq:\n",
    "                    barcodes_found.append(b[0])\n",
    "        barcode_counts = collections.Counter(barcodes_found)\n",
    "        all_barcodes.append(barcode_counts)\n",
    "    return all_barcodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barcode_counts(barcode_list):\n",
    "    \"\"\" \n",
    "    Take the collections.Counter objects generated by find_barcodes \n",
    "    and write a csv of the barcodes identified in that sample \n",
    "    \n",
    "    Args:\n",
    "        barcode_list: the list of collections.Counter objects \n",
    "    \n",
    "    Returns:\n",
    "        a csv written into the working directory of the barcodes identified and their count \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for sample in barcode_list:\n",
    "        if len(sample) > 0:\n",
    "            \n",
    "            total_counts = sum(sample.values())\n",
    "            num_barcodes = len(sample)\n",
    "            sampledict = dict(sample)\n",
    "            print(list(sampledict.keys()), sampledict.values())\n",
    "            #for s in range(0, num_barcodes):\n",
    "            #    print(dict(sample)[s])\n",
    "            print(\"number of unique barcodes:\", num_barcodes)\n",
    "            print(sample.keys())\n",
    "        \n",
    "    return total_counts, num_barcodes\n",
    "\n",
    "barcode_counts(bcs_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------\n",
    "## Simpson's Diversity Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Global Variables: User Input Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "######INFORMATION ABOUT THE DATA YOU ARE INPUTTING########\n",
    "\n",
    "#what is the absolute path to the directory that contains all of your data? \n",
    "appRoot = \"/Users/agolfinos/Desktop/Prophylactic_Vaccines_Barcode_Data/rh2909_rh2911_rh2921\"\n",
    "\n",
    "#FOLDER NAMES\n",
    "\n",
    "rawBarcodes = appRoot + \"/3.BarcodeData/\"\n",
    "\n",
    "splittingBarcodeAnalysisOutputDirectory = appRoot + \"/4.SplitBarcodeFiles\"\n",
    "\n",
    "summarySpreadsheetDirectory = appRoot + \"/5.AllDataToBeUsed\"\n",
    "\n",
    "sdiDirectory = appRoot + \"/6.CalculatingSDI\"\n",
    "\n",
    "sdiFinalSpreadsheets = appRoot + \"/7.SDI\"\n",
    "\n",
    "#mhFolder = appRoot + \"/8.Morisita-Horn\"\n",
    "\n",
    "\n",
    "#REPLICATES FOR THE REFERENCE AND EXPERIMENTAL SAMPLING STEPS \n",
    "\n",
    "replicates = 10\n",
    "\n",
    "\n",
    "#STATISTICS SPREADSHEETS\n",
    "\n",
    "animal_1_folders = glob.glob(summarySpreadsheetDirectory + \"/BarcodeList/\" + \"*\")\n",
    "\n",
    "\n",
    "#MORISITA-HORN FILES\n",
    "\n",
    "#animal1MH = mhFolder + \"/MHmatrix\" + animal1name + \".csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spreadsheet Handling and File Manipulation: Creating Necessary Directories\n",
    "\n",
    "This portion of the code makes all of the necessary directories for you to properly run this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_directories():    \n",
    "    if not os.path.exists(splittingBarcodeAnalysisOutputDirectory):\n",
    "        os.makedirs(splittingBarcodeAnalysisOutputDirectory)\n",
    "    \n",
    "    if not os.path.exists(summarySpreadsheetDirectory): \n",
    "        os.makedirs(summarySpreadsheetDirectory)\n",
    "        \n",
    "    if not os.path.exists(sdiDirectory): \n",
    "        os.makedirs(sdiDirectory)\n",
    "        \n",
    "    if not os.path.exists(sdiFinalSpreadsheets): \n",
    "        os.makedirs(sdiFinalSpreadsheets)\n",
    "        \n",
    "    #if not os.path.exists(mhFolder): \n",
    "        #os.makedirs(mhFolder)\n",
    "        \n",
    "directories = making_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying Files To Splitting Barcodes Directory\n",
    "\n",
    "This portion of the script will copy everything from the barcode analysis directory and copy it into the splitting barcodes directory so we can run the next portion of the script, which splits these files into two parts. We want to copy these files into a new directory instead of using the old directory because we want to save the original files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_barcode_analysis_files(): \n",
    "    \n",
    "    barcodes = os.listdir(rawBarcodes)\n",
    "    destpath = splittingBarcodeAnalysisOutputDirectory\n",
    "    for f in barcodes:\n",
    "        shutil.copy(rawBarcodes + \"/\" + f, destpath)\n",
    "            \n",
    "moving_files = moving_barcode_analysis_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Barcode Analysis Output Files\n",
    "\n",
    "This is the first step in the post-barcode analysis pipeline. It will take the output files and split them into two parts. This is because the original file has some \"summary stats\" at the top with certain column headers, and the second half of the sheet has different information and different column headers. Since this can be kind of a pain when we want to analyze this going forward, we split these files into two parts to prepare for the later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call all of the csv files in the cwd and makes a list of all of them\n",
    "def listfilenames(): #this writes all file names to a list\n",
    "    results = []\n",
    "    for root, dirs, files in os.walk(splittingBarcodeAnalysisOutputDirectory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.csv'): #this will be where we store the names of our files\n",
    "                results.append(filename)\n",
    "    return(results)\n",
    "\n",
    "def move_rows(results):\n",
    "    for fname in results:\n",
    "        file_in = splittingBarcodeAnalysisOutputDirectory + \"/\" + fname\n",
    "        file_out = splittingBarcodeAnalysisOutputDirectory + \"/\" + \"SUMMARY_\"  + fname\n",
    "        file_temp = splittingBarcodeAnalysisOutputDirectory + \"/\" + \"TEMP_\" + fname\n",
    "        with open(file_in, \"r\") as f_input, open(file_out, \"a\") as f_output, open(file_temp, \"w\") as f_temp:\n",
    "            csv_input = csv.reader(f_input)\n",
    "            \n",
    "            #append first 4 rows to file_out\n",
    "            csv.writer(f_output).writerows(itertools.islice(csv_input, 0, 4))\n",
    "            \n",
    "            #write the remaining rows from file_in to file_temp\n",
    "            csv.writer(f_temp).writerows(csv_input)\n",
    "        os.remove(file_in)\n",
    "        os.rename(file_temp, splittingBarcodeAnalysisOutputDirectory + \"/\" \"REVISED_STATS_\" + fname)\n",
    "\n",
    "            \n",
    "results = listfilenames()\n",
    "moving_rows = move_rows(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised Stats Spreadsheet Manipulation\n",
    "\n",
    "This step will first move all of the revised stats files into the summary spreadsheet directory. From there, we use the file names from each of the revised stats files (edited so we are only using a portion of the original name) to create directories named after each of the files. Then, after it creates a directory named after each of the files, it moves the file into the folder of the matching name. This is so that each file has its own directory to manage all of the summary sheets that are created in the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moving these files to a new folder \n",
    "def relocate_revised_stats():\n",
    "    source = splittingBarcodeAnalysisOutputDirectory\n",
    "    dest1 = summarySpreadsheetDirectory\n",
    "    files = os.listdir(source)\n",
    "    for f in files: \n",
    "        if f.startswith(\"REVISED_STATS\"):\n",
    "            full_file_name = os.path.join(source, f)\n",
    "            if (os.path.isfile(full_file_name)):\n",
    "                shutil.copy(full_file_name, dest1)\n",
    "\n",
    "#makes a list of all directories so we can use this to make directories \n",
    "def make_directories_list():\n",
    "    for root, dirs, files in os.walk(summarySpreadsheetDirectory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.csv'): #this will be where we store the names of our files\n",
    "                \n",
    "                #########ALTER YOUR NAME SELECTION CRITERIA HERE##############\n",
    "                f = re.split('_', filename)\n",
    "                f1 = f[2:5] #these only keep the part of the name that you need and omits the other parts of the name\n",
    "                f2 = '_'.join(f1)\n",
    "                ###############################################################\n",
    "                \n",
    "                if not os.path.exists(summarySpreadsheetDirectory + \"/\" + f2):\n",
    "                    os.makedirs(summarySpreadsheetDirectory + \"/\" + f2) #MAKES DIRECTORY FOR THE FILE\n",
    "                shutil.move(summarySpreadsheetDirectory + \"/\" + filename, summarySpreadsheetDirectory + \"/\" + f2) #MOVES THE NAME MATCHED FILE INTO ITS CORRESPONDING DIRECTORY \n",
    "\n",
    "relocating_revised_stats = relocate_revised_stats()\n",
    "making_directories_list = make_directories_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Summary Spreadsheets 1-3\n",
    "\n",
    "This is the part of the code that will take the split and relocated files from the barcode virus analysis tool and will turn them into 3 separate spreadsheets that we will use as part of the rest of this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prevent the setting with copy warning by disabling chained assignment\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def identify_barcodes(f2):\n",
    "    \n",
    "    #creates an empty pandas dataframe to read the results to\n",
    "    df = pd.DataFrame([])\n",
    "    \n",
    "    #looks only for files that start with REVISED_STATS and ends with csv\n",
    "    for counter, file in enumerate(glob.glob(\"REVISED_STATS*.csv\")):\n",
    "        \n",
    "        #this only uses the first and third columns of the csv file, sets the index column as the barcode name\n",
    "        namedf = pd.read_csv(file, usecols=[0,2])\n",
    "        df = df.append(namedf)\n",
    "        \n",
    "        if file.endswith('.csv'): #this will be where we store the names of our files\n",
    "            \n",
    "            #############ALTER YOUR NAMING SCHEME HERE####################\n",
    "            f = file[22:]\n",
    "            f1 = f[:-37] #these only keep the part of the name that you need and omits the other parts of the name\n",
    "            f2 = str(f1)\n",
    "            ##############ALTER YOUR NAMING SCHEME HERE##################\n",
    "        \n",
    "    #filters out unique barcodes and only keeps the SIV barcodes\n",
    "    df1 = df[df[\"Barcode\"].str.contains(\"SIV\", na=False)]\n",
    "    \n",
    "    #here we sum up the total barcode count\n",
    "    total_count = df1[\"Counts\"].sum()\n",
    "    \n",
    "    #here we add the column with the total barcode count\n",
    "    df1[\"Total_Barcode_Count\"] = total_count\n",
    "    \n",
    "    #here we create the frequency column\n",
    "    df1['Percent_Composition'] = df1['Counts']/df1['Total_Barcode_Count']\n",
    "    \n",
    "    #creates a data frame with the raw counts of all the barcodes found in at least 0.1% proportion\n",
    "    df2 = df1[[\"Barcode\", \"Counts\"]]\n",
    "    \n",
    "    #prints the filtered barcode count data to a csv\n",
    "    df2.to_csv(\"1_\" + str(f2) + \"_Filtered_Barcode_Counts.csv\", index=False)\n",
    "    \n",
    "    #read in the source file to a pandas dataframe\n",
    "    fname = \"1_\" + str(f2) + \"_Filtered_Barcode_Counts.csv\"\n",
    "    df = pd.read_csv(fname)\n",
    "    \n",
    "    #here we sum up the total barcode count\n",
    "    total_count = df[\"Counts\"].sum()\n",
    "    \n",
    "    #here we add the column with the total barcode count\n",
    "    df[\"Total_Barcode_Count\"] = total_count\n",
    "    \n",
    "    #here we create the frequency column\n",
    "    df['Percent_Composition'] = df['Counts']/df['Total_Barcode_Count']\n",
    "    \n",
    "    #making a new column that includes the path name \n",
    "    idx = 0\n",
    "    new_col = path\n",
    "    df.insert(loc=idx, column=\"Sample_Name\", value=new_col)\n",
    "    \n",
    "    #making a dataframe that only has proportion so we are able to plot this\n",
    "    df1 = df[[\"Barcode\",\"Percent_Composition\"]]\n",
    "    \n",
    "    #printing the dataframes to their own spreadsheets\n",
    "    df.to_csv(\"2_\" + path + \"_Filtered_Barcode_Counts_and_Percentage.csv\", index=False)\n",
    "    df1.to_csv(\"3_\" + path + \"_Filtered_Barcode_Percentage.csv\", index=False)\n",
    "    \n",
    "#executing workflow\n",
    "#this automates the running of the code by iterating through the directories\n",
    "os.chdir(summarySpreadsheetDirectory)\n",
    "for i in os.listdir(summarySpreadsheetDirectory):\n",
    "    \n",
    "    #ignores hidden directory .DS_Store\n",
    "    if i == \".DS_Store\":\n",
    "        continue\n",
    "        \n",
    "    print(i)\n",
    "    \n",
    "    #changes directory to the next one in the list\n",
    "    os.chdir(i)\n",
    "    \n",
    "    #changes the path variable to the directory you just moved into\n",
    "    path = i\n",
    "    \n",
    "    #print an update statement\n",
    "    print(\"Starting analysis for the \" + path + \" directory\")\n",
    "    \n",
    "    #execution of the functions\n",
    "    listing_barcodes = identify_barcodes(making_directories_list)\n",
    "    \n",
    "    #printing a update statement\n",
    "    print(\"Finished analysis for the \" + path + \" directory\\n\\n\")\n",
    "    \n",
    "    #moves the directory back up to the original directory\n",
    "    os.chdir(\"../\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpson's Diversity Index Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############DEFINING THE FUNCTION#####################################################################################\n",
    "\n",
    "def SDI(): \n",
    "    \n",
    "###########NOW OPENING THE FIRST FILE TO EXTRACT THE BARCODES AND ADD THEM TO THE TOTAL POOL##########################    \n",
    "    \n",
    "    SDI_file_names = []\n",
    "    \n",
    "    SDImedians = []\n",
    "    \n",
    "    for roots, dirs, files in os.walk(summarySpreadsheetDirectory): \n",
    "            \n",
    "        for dir in dirs: \n",
    "            \n",
    "            all_barcodes1 = []\n",
    "        \n",
    "            #splitting the path name so we can make a list of all the samples as we iterate through the files\n",
    "            split_i = re.split('/', summarySpreadsheetDirectory + \"/\" + dir)\n",
    "            sampleName = split_i[-1]\n",
    "\n",
    "            #adding the file name to the list of filenames to use later for an index column and row\n",
    "            SDI_file_names.append(sampleName)\n",
    "\n",
    "            #creates a list of all of the files in this directory\n",
    "            directory = os.listdir(summarySpreadsheetDirectory + \"/\" + dir)\n",
    "\n",
    "            #filtering the list for only the second spreadsheet, which is the only one we will be using\n",
    "            first_file = [s for s in directory if s.startswith(\"1_\")]\n",
    "\n",
    "            #converting the file name to a string\n",
    "            string_first_file = ''.join(first_file)\n",
    "            \n",
    "            path = str(summarySpreadsheetDirectory + \"/\" + dir)\n",
    "\n",
    "            #gives us the absolute full path to the first spreadsheet in that directory\n",
    "            string_path_1 = os.path.join(path, string_first_file)\n",
    "\n",
    "            #converts this file to a pandas dataframe\n",
    "            df1 = pd.read_csv(string_path_1, header=[0])\n",
    "            \n",
    "            #iterates through rows of the pandas dataframe\n",
    "            for index, row in df1.iterrows(): \n",
    "\n",
    "                #creates a \"counts\" value that gives the number of times that this barcode is found in that file\n",
    "                counts1 = row['Counts']\n",
    "                counts1 = int(counts1)\n",
    "\n",
    "                #gives us the name of the barcode associated with the count right above\n",
    "                barcode1 = row['Barcode']\n",
    "\n",
    "                #this appends all the barcode names to the list in the range that is given in the counts column\n",
    "                all_barcodes1.extend([barcode1 for x in range(counts1)])\n",
    "\n",
    "            #convert all_barcodes to a numpy array \n",
    "            array1 = np.array(all_barcodes1)\n",
    "\n",
    "###########NOW SETTING THE LOOP THAT WILL RUN THIS PIPELINE IN REPLICATE##############################################\n",
    "            \n",
    "            #this is going to create a list of all SDI values creates during this step so we can find the median\n",
    "            #value and make that the final value \n",
    "            SDIreplicates = []\n",
    "        \n",
    "            #this is going to run the \"picking\" from the all_barcodes array a fixed number of times (\"replicates\" value)\n",
    "            for x in range(0, replicates):\n",
    "                \n",
    "###########NOW RUNNING SDI CALCULATION ON THIS DATAFRAME##############################################################\n",
    "                \n",
    "                #this only uses the counts column \n",
    "                #f2 = f1['Counts']\n",
    "                f2 = df1['Counts']\n",
    "                \n",
    "                #this turns the column into a numpy array to convert them into integers\n",
    "                a = np.array(f2)\n",
    "                \n",
    "                #sums the counts column (should match the number of reads we pulled out)\n",
    "                N = df1['Counts'].sum()\n",
    "                \n",
    "                #creates a list to turn into the number for the numerator\n",
    "                numerator = 0\n",
    "                \n",
    "                #creates the sum to be used in the numerator\n",
    "                for i in a: \n",
    "                    numerator = numerator + i*(i-1)\n",
    "                \n",
    "                #this actually calculates the Simpson diversity index\n",
    "                SDI = float(1-(numerator/(N*(N-1))))\n",
    "                \n",
    "                #this appends the SDI value to the list\n",
    "                SDIreplicates.append(SDI)\n",
    "            \n",
    "            #this finds the median value from the list of replicates \n",
    "            SDImedian = statistics.median(SDIreplicates)\n",
    "            \n",
    "            #adding this value to the list of SDI medians\n",
    "            SDImedians.append(SDImedian)\n",
    "            \n",
    "            #sets the new file name equal to a variable so it can be created later \n",
    "            fname = sdiDirectory + \"/\" + 'Simpson_Diversity_Index_allreads_' + str(replicates) + \"replicates.csv\"\n",
    "            \n",
    "    #zips the SDI file names to the corresponding median values to use to make a spreadsheet\n",
    "    SDI_values = dict(zip(SDI_file_names, SDImedians))\n",
    "\n",
    "    #converts this first dictionary to a pandas dataframe so we can save it as a csv file\n",
    "    SDI_df = pd.DataFrame(SDI_values.items(), columns=['Time_Point', 'SDI_Value'])\n",
    "    \n",
    "    #save this dataframe to a csv \n",
    "    SDI_df.to_csv(fname, columns=['Time_Point', 'SDI_Value'])\n",
    "    \n",
    "    SDI_df.to_csv(sdiFinalSpreadsheets + \"/\" + \"SDI_allreads_\" + str(replicates) + \"reps_.csv\", index=False)\n",
    "    \n",
    "    \n",
    "Simpsons = SDI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
